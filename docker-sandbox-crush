#!/bin/bash

set -e

VERSION="0.12.1"
CRUSH_VERSION="0.34.0"
# Docker image configuration - can be overridden via DOCKER_SANDBOX_IMAGE environment variable
DOCKER_IMAGE="${DOCKER_SANDBOX_IMAGE:-node:22-bookworm}"

# Resource configuration - can be overridden via environment variables
# Tested defaults for 3-5 containers in parallel on a Macbook Pro with 32GB RAM
DOCKER_SANDBOX_MEMORY="${DOCKER_SANDBOX_MEMORY:-4g}"
DOCKER_SANDBOX_CPUS="${DOCKER_SANDBOX_CPUS:-2.0}"
DOCKER_SANDBOX_NPROC="${DOCKER_SANDBOX_NPROC:-1024}"

export DOCKER_CLI_HINTS=off

# Color codes (disabled if NO_COLOR is set or not a TTY)
if [ -t 0 ] && [ -z "$NO_COLOR" ]; then
    GREEN='\033[0;32m'
    RED='\033[0;31m'
    YELLOW='\033[1;33m'
    CYAN='\033[0;36m'
    GRAY='\033[0;90m'
    NC='\033[0m'
else
    GREEN=''
    RED=''
    YELLOW=''
    CYAN=''
    GRAY=''
    NC=''
fi

print_success() {
    echo -e "${GREEN}✓${NC} $1"
}

print_error() {
    echo -e "${RED}✗${NC} $1" >&2
}

print_warning() {
    echo -e "${YELLOW}!${NC} $1"
}

print_info() {
    echo -e "${CYAN}${1}${NC}"
}

print_dry_run() {
    echo -e "${YELLOW}[DRY RUN]${NC} $1"
}

print_usage() {
    echo "Usage: crush-sandbox <command> [options]"
    echo ""
    echo "Commands:"
    echo "  run                     Run Crush CLI in Docker sandbox"
    echo "  clean                   Remove the sandbox container for current workspace"
    echo "  help                    Show this help message"
    echo "  install                 Install this script to /usr/local/bin"
    echo "  update                  Update to the latest version"
    echo "  uninstall               Remove all docker-sandbox-crush artifacts"
    echo "  list-containers         List all containers for current repository"
    echo "  list-worktrees          List all git worktrees in current workspace"
    echo "  remove-worktree <name>  Remove a git worktree"
    echo ""
    echo "Options:"
    echo "  --version               Show version information"
    echo "  --force                 Skip confirmation prompts"
    echo "  --shell                 Start interactive shell instead of Crush CLI (for debugging)"
    echo "  --no-host-config         Skip mounting host Crush config directory"
    echo "  --cred-scan             Enable credential scanning before starting container"
    echo "  --dry-run               Preview uninstall actions without executing"
    echo "  --worktree [name]       Create a worktree with optional name (defaults to worktree name as branch name)"
    echo "  --branch-name [name]    Specify branch name for worktree (skips branch prompt, requires --worktree)"
    echo "  --model [model]         Specify AI model for programmatic mode (e.g., 'openai/gpt-4', 'claude-3.5')"
    echo "  -p \"prompt\"           Send prompt directly to Crush CLI (programmatic mode)"
    echo "  --docker-args \"args\"  Pass additional Docker arguments (advanced, use with caution)"
    echo ""
    echo "    Examples:"
    echo "      --docker-args \"--network host\""
    echo "      --docker-args \"-v /path/to/data:/data\""
    echo "      --docker-args \"--cap-add NET_ADMIN\""
    echo ""
    echo "    Security Note:"
    echo "      Docker arguments are passed directly to Docker CLI without validation."
    echo "      You are responsible for ensuring arguments are secure and appropriate."
    echo "      This can override existing security controls (e.g., capability dropping)."
}

validate_docker() {
    # Check if Docker CLI is installed
    if ! command -v docker &> /dev/null; then
        echo "Error: docker CLI is not installed or not in PATH" >&2
        return 1
    fi

    # Check if Docker daemon is running and accessible
    if ! docker info &> /dev/null; then
        echo "Error: docker daemon is not running or not accessible" >&2
        return 1
    fi

    return 0
}

validate_workspace_path() {
    local workspace="$1"

    # Check for problematic shell metacharacters that could cause command injection
    # Use grep for reliable pattern matching

    # Check for dollar sign
    if echo "$workspace" | grep -q '\$'; then
        echo "Error: Workspace path contains invalid character '\$': $workspace" >&2
        exit 1
    fi

    # Check for backtick
    if echo "$workspace" | grep -q '\`'; then
        echo "Error: Workspace path contains invalid character 'backtick': $workspace" >&2
        exit 1
    fi

    # Check for backslash
    if echo "$workspace" | grep -q '\\'; then
        echo "Error: Workspace path contains invalid character 'backslash': $workspace" >&2
        exit 1
    fi

    # Check for semicolon
    if echo "$workspace" | grep -q ';'; then
        echo "Error: Workspace path contains invalid character ';': $workspace" >&2
        exit 1
    fi

    # Check for pipe
    if echo "$workspace" | grep -q '|'; then
        echo "Error: Workspace path contains invalid character '|': $workspace" >&2
        exit 1
    fi

    # Check for ampersand
    if echo "$workspace" | grep -q '&'; then
        echo "Error: Workspace path contains invalid character '&': $workspace" >&2
        exit 1
    fi

    # Check for less than
    if echo "$workspace" | grep -q '<'; then
        echo "Error: Workspace path contains invalid character '<': $workspace" >&2
        exit 1
    fi

    # Check for greater than
    if echo "$workspace" | grep -q '>'; then
        echo "Error: Workspace path contains invalid character '>': $workspace" >&2
        exit 1
    fi

    # Check for command substitution pattern $(...)
    if echo "$workspace" | grep -q '\$('; then
        echo "Error: Workspace path contains command substitution pattern: $workspace" >&2
        exit 1
    fi

    return 0
}

get_workspace_info() {
    local workspace
    workspace="$(pwd)"

    # Normalize workspace to worktree root if inside a worktree
    # This ensures consistent container naming regardless of subdirectory depth
    case "$workspace" in
        */.worktrees/*)
            # Extract path up to and including worktree name
            # /repo/.worktrees/feature/src/comp -> /repo/.worktrees/feature
            local before_worktrees
            local worktree_name
            local after_worktrees

            before_worktrees="${workspace%%/.worktrees/*}"
            after_worktrees="${workspace#*/.worktrees/}"
            worktree_name="${after_worktrees%%/*}"

            workspace="${before_worktrees}/.worktrees/${worktree_name}"
            ;;
    esac

    echo "$workspace"
}

read_git_config() {
    local gitconfig="$HOME/.gitconfig"
    local user_name=""
    local user_email=""

    if [ -f "$gitconfig" ]; then
        user_name="$(git config --file "$gitconfig" user.name 2>/dev/null || echo "")"
        user_email="$(git config --file "$gitconfig" user.email 2>/dev/null || echo "")"
    else
        echo "Warning: ~/.gitconfig not found" >&2
    fi

    if [ -z "$user_name" ] && [ -z "$user_email" ]; then
        echo "Warning: Git user.name and user.email not configured" >&2
    fi

    echo "$user_name|$user_email"
}

get_host_crush_config_path() {
    local config_path=""

    # Check ~/.config/crush/
    local config_dir_1="$HOME/.config/crush"
    if [ -d "$config_dir_1" ]; then
        if [ -r "$config_dir_1" ]; then
            config_path="$config_dir_1"
        else
            echo "Error: Crush config directory exists but is not readable: $config_dir_1" >&2
            exit 1
        fi
    fi

    # Log warning if no config directory found (but continue execution)
    if [ -z "$config_path" ]; then
        echo "Warning: No host Crush config directory found (checked ~/.config/crush/)" >&2
    fi

    echo "$config_path"
}

get_host_session_data_path() {
    local session_data_path=""

    # Check ~/.local/share/crush/
    local data_dir="$HOME/.local/share/crush"
    if [ -d "$data_dir" ]; then
        if [ -r "$data_dir" ]; then
            session_data_path="$data_dir"
        else
            echo "Error: Crush session data directory exists but is not readable: $data_dir" >&2
            exit 1
        fi
    fi

    # Log warning if no session data directory found (but continue execution)
    if [ -z "$session_data_path" ]; then
        echo "Warning: No host Crush session data directory found (checked ~/.local/share/crush/)" >&2
    fi

    echo "$session_data_path"
}

ensure_gitleaks_image() {
    local gitleaks_image="ghcr.io/gitleaks/gitleaks:latest"

    # Check if gitleaks image exists locally (fast path)
    if docker images --format '{{.Repository}}:{{.Tag}}' | grep -q "^${gitleaks_image}$"; then
        return 0
    fi

    echo "Pulling gitleaks Docker image for credential scanning..."
    if docker pull "$gitleaks_image" > /dev/null 2>&1; then
        echo "✓ gitleaks Docker image pulled successfully"
        return 0
    else
        echo "Warning: Failed to pull gitleaks Docker image" >&2
        echo "  Credential scanning will be unavailable" >&2
        return 1
    fi
}

scan_credentials() {
    local workspace="$1"
    local enable_scan="$2"
    local gitleaks_image="ghcr.io/gitleaks/gitleaks:latest"

    # Skip scanning unless explicitly enabled
    if [ "$enable_scan" != "true" ]; then
        return 0
    fi

    # Ensure gitleaks image is available
    ensure_gitleaks_image

    # Check if gitleaks image exists locally (may have failed to pull)
    if ! docker images --format '{{.Repository}}:{{.Tag}}' | grep -q "^${gitleaks_image}$"; then
        echo "Warning: gitleaks Docker image not available - credential scanning skipped" >&2
        return 0
    fi

    echo "Scanning workspace for credentials..."

    # Run gitleaks scan using Docker image
    local scan_output
    local exit_code
    
    # Capture output and exit code with verbose error handling
    echo "Running: docker run --rm -v \"${workspace}:/path\" \"$gitleaks_image\" directory /path --no-banner" >&2
    
    # Temporarily disable 'set -e' to capture exit code without script termination
    set +e
    scan_output=$(docker run --rm -v "${workspace}:/path" "$gitleaks_image" directory /path --no-banner 2>&1)
    exit_code=$?
    set -e
    
    echo "Gitleaks exit code: $exit_code" >&2
    if [ -n "$scan_output" ]; then
        echo "Gitleaks output:" >&2
        echo "$scan_output" >&2
    fi

    if [ $exit_code -eq 0 ]; then
        echo "✓ No credentials detected"
    elif [ $exit_code -eq 1 ]; then
        echo "⚠️  WARNING: Potential credentials detected in workspace!"
        echo ""
        echo "⚠️  WARNING: Potential credentials detected in workspace!"
        echo ""
        echo "Found files that may contain secrets or credentials."
        echo "For your security, please:"
        echo "  1. Review and remove any API keys, passwords, or tokens"
        echo "  2. Add sensitive files to .gitignore"
        echo "  3. Rotate any exposed credentials"
        echo ""
        echo ""

        # Prompt user to continue or abort
        read -p "Continue anyway? [y/N] " confirm
        if [[ ! "$confirm" =~ ^[yY] ]]; then
            echo "Aborted"
            exit 0
        fi
        echo "Proceeding with credential warning..."
    else
        echo "ERROR: Something went wrong during credential scan."
        echo "$scan_output"
    fi

    return 0
}
get_container_name() {
    local root_workspace="$1"
    local worktree_name="$2"

    # Compute repository hash from root workspace path
    local repo_hash
    repo_hash="$(echo -n "$root_workspace" | shasum -a 256 | awk '{print $1}' | cut -c1-12)"

    # Generate container name based on workspace type
    if [ -n "$worktree_name" ]; then
        # Worktree: crush-sandbox-<repo-hash>-<worktree-name>
        echo "crush-sandbox-${repo_hash}-${worktree_name}"
    else
        # Main workspace: crush-sandbox-<repo-hash>
        echo "crush-sandbox-${repo_hash}"
    fi
}

get_cache_volume_name() {
    local workspace="$1"
    # Use SHA256 hash of workspace path for deterministic volume name
    # Truncated to first 12 characters for reasonable length
    # Prefix with 'crush-cache-' for readability
    local hash
    hash="$(echo -n "$workspace" | shasum -a 256 | awk '{print $1}' | cut -c1-12)"
    echo "crush-cache-${hash}"
}

container_exists() {
    local container_name="$1"
    # Use docker ps -a to check if container exists (including stopped containers)
    # --filter name=... matches containers with the exact name
    docker ps -a --filter "name=^${container_name}$" --format '{{.Names}}' | grep -q "^${container_name}$"
    return $?
}

get_repository_containers() {
    local root_workspace="$1"

    # Compute repository hash from root workspace path
    local repo_hash
    repo_hash="$(echo -n "$root_workspace" | shasum -a 256 | awk '{print $1}' | cut -c1-12)"

    # Find all containers for this repository (prefix: crush-sandbox-<repo-hash>)
    local containers
    containers=$(docker ps -a --filter "name=^crush-sandbox-${repo_hash}" --format '{{.Names}}' 2>/dev/null || echo "")

    echo "$containers"
}

create_cache_volume() {
    local cache_volume_name="$1"

    # Check if volume already exists
    if docker volume ls -q --filter "name=^${cache_volume_name}$" | grep -q "^${cache_volume_name}$"; then
        echo "Using existing cache volume: $cache_volume_name"
        return 0
    fi

    # Create the volume
    echo "Creating cache volume: $cache_volume_name"
    docker volume create "$cache_volume_name" > /dev/null

    # Fix permissions for node user (UID 1000) so it can write to the cache
    docker run --rm -v "${cache_volume_name}:/workspace-cache" "${DOCKER_IMAGE}" chown -R 1000:1000 /workspace-cache > /dev/null 2>&1 || true

    echo "Cache volume created: $cache_volume_name"
}

create_container() {
    local workspace="$1"
    local container_name="$2"
    local git_name="$3"
    local git_email="$4"
    local cache_volume_name="$5"
    local host_config_path="$6"
    local host_session_data_path="$7"
    # Docker args are inserted into the docker_args array before the image name
    # This allows users to pass arbitrary Docker arguments for advanced customization
    # Args are stored as a base64-encoded label for persistence across container restarts
    local extra_docker_args="$8"
    local docker_args_source="$9"

    local base_image="${DOCKER_IMAGE}"
    local docker_args=(
        "--name" "$container_name"
        "-v" "${workspace}:${workspace}"
        "-v" "${cache_volume_name}:/workspace-cache"
        "-w" "$workspace"
        "--user" "1000:1000"
        "--memory=${DOCKER_SANDBOX_MEMORY}"
        "--memory-swap=${DOCKER_SANDBOX_MEMORY}"
        "--cpus=${DOCKER_SANDBOX_CPUS}"
        "--ulimit" "nproc=${DOCKER_SANDBOX_NPROC}"
        # Security: Drop all capabilities, then add back only what's needed
        "--cap-drop" "ALL"
        "--cap-add" "CHOWN"
        "--cap-add" "DAC_OVERRIDE"
    )

    # Mount host Crush config read-only if available
    if [ -n "$host_config_path" ]; then
        docker_args+=("-v" "${host_config_path}:/host-crush-config:ro")
    fi

    # Mount host session data read-only if available
    if [ -n "$host_session_data_path" ]; then
        docker_args+=("-v" "${host_session_data_path}:/host-session-data:ro")
    fi

    # Configure npm cache to use persistent volume
    docker_args+=("-e" "npm_config_cache=/workspace-cache/npm")

    # Add environment variables for Git config if available
    if [ -n "$git_name" ]; then
        docker_args+=("-e" "GIT_USER_NAME=${git_name}")
    fi
    if [ -n "$git_email" ]; then
        docker_args+=("-e" "GIT_USER_EMAIL=${git_email}")
    fi

    # Add extra Docker args if provided (inserted before security flags)
    if [ -n "$extra_docker_args" ]; then
        # Base64-encode the args and store as label for persistence
        local encoded_args
        encoded_args=$(echo -n "$extra_docker_args" | base64)
        docker_args+=("--label" "crush-sandbox.extra-docker-args=${encoded_args}")

        # Store source metadata (env var, CLI, or both)
        if [ -n "$docker_args_source" ]; then
            docker_args+=("--label" "crush-sandbox.extra-docker-args-source=${docker_args_source}")
        fi

        # Parse and add extra Docker args to docker_args array
        # Use eval to split args properly (spaces, quotes, etc.)
        # Security: Users are responsible for Docker argument validation
        eval "local extra_args_array=($extra_docker_args)"
        for arg in "${extra_args_array[@]}"; do
            docker_args+=("$arg")
        done
        echo "Using custom Docker args: $extra_docker_args (source: ${docker_args_source})"
    fi

    docker_args+=("$base_image")
    # Use tail -f /dev/null to keep container running
    docker_args+=("tail" "-f" "/dev/null")

    echo "Creating container from image: ${base_image}"
    docker create "${docker_args[@]}"
}

setup_crush_script() {
    local container_name="$1"

    # Create the setup script inside the container in /home/node/ where node user has write access
    docker exec "$container_name" sh -c 'cat > /home/node/setup-crush.sh << ENDSCRIPT
#!/bin/sh

# Configure npm to use node user home directory
mkdir -p /home/node/.npm-global
npm config set prefix /home/node/.npm-global

# Add npm global bin to PATH for this script session
export PATH=/home/node/.npm-global/bin:\$PATH

# Check if Crush CLI is already installed
if command -v crush >/dev/null 2>&1; then
    echo "Crush CLI is already installed"
    exit 0
fi

# Install Crush CLI via npm global install
echo "Installing Crush CLI..."
if npm install -g @charmland/crush@"$CRUSH_VERSION" >/dev/null 2>&1; then
    echo "Crush CLI installed successfully"
    exit 0
else
    echo "Error: Failed to install Crush CLI"
    exit 1
fi
ENDSCRIPT' >/dev/null 2>&1

    # Make the script executable
    docker exec "$container_name" chmod +x /home/node/setup-crush.sh >/dev/null 2>&1
}

setup_crush_config_script() {
    local container_name="$1"

    # Create config merge script inside the container in /home/node/
    docker exec "$container_name" sh -c 'cat > /home/node/setup-crush-config.sh << ENDSCRIPT
#!/bin/sh

# Create separate writable directories for config and data
mkdir -p /tmp/crush-config/config
mkdir -p /tmp/crush-config/data

# Copy host config to writable location (from read-only mount)
if [ -d /host-crush-config ]; then
    cp -r /host-crush-config/* /tmp/crush-config/config/ 2>/dev/null || true
    echo "Copied host configuration from /host-crush-config to /tmp/crush-config/config"
fi

# Copy session data to writable location (from read-only mount)
if [ -d /host-session-data ]; then
    cp -r /host-session-data/* /tmp/crush-config/data/ 2>/dev/null || true
    echo "Copied session data from /host-session-data to /tmp/crush-config/data"
fi

# Set environment variables to point to separate writable directories
export CRUSH_GLOBAL_CONFIG=/tmp/crush-config/config
export CRUSH_GLOBAL_DATA=/tmp/crush-config/data
echo "CRUSH_GLOBAL_CONFIG set to: \$CRUSH_GLOBAL_CONFIG"
echo "CRUSH_GLOBAL_DATA set to: \$CRUSH_GLOBAL_DATA"
ENDSCRIPT' >/dev/null 2>&1

    # Make the script executable
    docker exec "$container_name" chmod +x /home/node/setup-crush-config.sh >/dev/null 2>&1
}

run_container() {
    local container_name="$1"
    local shell_mode="$2"
    local host_config_path="$3"
    local workspace="$4"
    local programmatic_mode="$5"
    local prompt="$6"
    local model="$7"

    # Start the container (or restart if it was stopped)
    echo "Starting container..."
    docker start "$container_name" > /dev/null

    # Setup Crush CLI script
    setup_crush_script "$container_name"
    setup_crush_config_script "$container_name"

    # Run Crush CLI setup script
    echo "Setting up Crush CLI..."
    if ! docker exec "$container_name" /home/node/setup-crush.sh; then
        echo "Warning: Failed to setup Crush CLI"
    fi

    # Setup configuration and set CRUSH_GLOBAL_CONFIG
    echo "Setup Crush configuration..."
    docker exec "$container_name" /home/node/setup-crush-config.sh

    # Install pnpm if not already installed (uses npm cache)
    if ! docker exec "$container_name" sh -c "export PATH=/home/node/.npm-global/bin:\$PATH && command -v pnpm >/dev/null 2>&1"; then
        echo "Installing pnpm..."
        docker exec "$container_name" sh -c "npm config set prefix /home/node/.npm-global && npm install -g pnpm" >/dev/null 2>&1
        echo "pnpm installed"
    fi

    # Display npm version for verification
    local npm_version
    npm_version=$(docker exec "$container_name" npm --version 2>/dev/null || echo "not available")
    echo "npm version: $npm_version"

    # Display pnpm version if installed
    local pnpm_version
    pnpm_version=$(docker exec "$container_name" sh -c "export PATH=/home/node/.npm-global/bin:\$PATH && if command -v pnpm >/dev/null 2>&1; then pnpm --version; fi" 2>/dev/null || echo "")
    if [ -n "$pnpm_version" ]; then
        echo "pnpm version: $pnpm_version"
    fi

    # Configure pnpm cache directory and store location (if pnpm is installed)
    docker exec "$container_name" sh -c "export PATH=/home/node/.npm-global/bin:\$PATH && if command -v pnpm >/dev/null 2>&1; then pnpm config set cache-dir /workspace-cache/pnpm/cache && pnpm config set store-dir /workspace-cache/pnpm/store && echo 'pnpm store configured to:' && pnpm store path; else echo 'pnpm not installed'; fi"

    # Execute command in container
    # Build docker exec args as an array to handle paths with spaces
    local docker_exec_args=()

    # Add working directory, to override any previous -w settings if worktree
    docker_exec_args+=("-w")
    docker_exec_args+=("$workspace")

    # Add CRUSH_GLOBAL_CONFIG
    docker_exec_args+=("-e")
    docker_exec_args+=("CRUSH_GLOBAL_CONFIG=/tmp/crush-config/config")

    # Add CRUSH_GLOBAL_DATA
    docker_exec_args+=("-e")
    docker_exec_args+=("CRUSH_GLOBAL_DATA=/tmp/crush-config/data")

    # Add PATH with npm global prefix so crush and pnpm can be found
    local container_path
    container_path=$(docker exec "$container_name" sh -c 'echo $PATH')
    docker_exec_args+=("-e")
    docker_exec_args+=("PATH=/home/node/.npm-global/bin:$container_path")

    if [ "$shell_mode" = "true" ]; then
        # Execute an interactive shell in the container (for debugging)
        echo "Starting interactive shell in sandbox..."
        if [ -t 0 ]; then
            # Interactive mode with TTY
            docker exec -it "${docker_exec_args[@]}" "$container_name" /bin/sh
        else
            # Non-interactive mode (for testing)
            echo "Warning: Not running in TTY mode, starting shell anyway..."
            docker exec -i "${docker_exec_args[@]}" "$container_name" /bin/sh
        fi
    elif [ "$programmatic_mode" = "true" ]; then
        # Programmatic mode: execute crush run with prompt
        # Don't use -i flag to avoid Crush reading from terminal stdin
        # Temporarily disable 'set -e' to capture exit code without script termination in case of invalid --model input
        set +e
        if [ -n "$model" ]; then
            echo "Executing: crush run --model \"$model\" \"$prompt\""
            docker exec "${docker_exec_args[@]}" "$container_name" crush run --model "$model" $prompt
        else
            echo "Executing: crush run \"$prompt\""
            docker exec "${docker_exec_args[@]}" "$container_name" crush run $prompt
        fi
        EXEC_EXIT_CODE=$?
        set -e

        echo "Crush CLI exited with code: $EXEC_EXIT_CODE"
        if [ $EXEC_EXIT_CODE -ne 0 ]; then
            echo "Error: Crush CLI exited with code: $EXEC_EXIT_CODE" >&2
            if [ -n "$model" ]; then
                echo "" >&2
                echo "Possible issues:" >&2
                echo "  - Invalid model name: '$model' may not be available" >&2
                echo "  - Check model name with Crush CLI documentation" >&2
                echo "  - Verify Crush CLI is properly installed" >&2
                echo "" >&2
            fi
            exit $EXEC_EXIT_CODE
        fi
    else
        # Interactive mode: execute crush without arguments
        echo "Starting Crush CLI..."
        if [ -t 0 ]; then
            # Interactive mode with TTY
            docker exec -it "${docker_exec_args[@]}" "$container_name" crush
        else
            # Non-interactive mode (for testing)
            echo "Warning: Not running in TTY mode, starting Crush CLI anyway..."
            docker exec -i "${docker_exec_args[@]}" "$container_name" crush
        fi
    fi

    # Stop the container after the session ends (but don't remove it)
    echo "Stopping container..."
    docker stop "$container_name" > /dev/null
    echo "Container stopped"
}

run_command() {
    # Validate Docker is available
    if ! validate_docker; then
        exit 1
    fi

    # Get workspace info (current directory)
    local workspace
    workspace="$(get_workspace_info)"

    # Get root workspace (git repository root - same for all worktrees)
    local root_workspace
    if ! validate_git_repository; then
        echo "Error: Not in a git repository" >&2
        exit 1
    fi
    root_workspace="$(get_root_workspace)"
    if [ -z "$root_workspace" ]; then
        echo "Error: Could not determine root workspace" >&2
        exit 1
    fi

    # Handle worktree creation mode
    local worktree_path=""
    local actual_worktree_name=""

    if [ "$WORKTREE_MODE" = "true" ]; then

        # Prepare .worktrees directory
        prepare_worktree_directory "$root_workspace"

        # Generate worktree name if not provided
        actual_worktree_name="$WORKTREE_NAME"
        if [ -z "$actual_worktree_name" ]; then
            actual_worktree_name="$(generate_worktree_name)"
            echo "Generated worktree name: $actual_worktree_name"
        fi

        # Validate worktree name (basic validation) to match docker container name rules
        if echo "$actual_worktree_name" | grep -q '[^a-zA-Z0-9_-]'; then
            echo "Error: Invalid worktree name '$actual_worktree_name'. Use only alphanumeric characters, hyphens, and underscores." >&2
            exit 1
        fi

        # Check if worktree already exists or create it
        if worktree_exists "$root_workspace" "$actual_worktree_name"; then
            echo "Using existing worktree: $actual_worktree_name"
        else
            if ! create_worktree "$root_workspace" "$actual_worktree_name" "$BRANCH_NAME"; then
                exit 1
            fi
        fi

        # Set worktree path
        worktree_path="${root_workspace}/.worktrees/${actual_worktree_name}"

        # Change to worktree directory
        cd "$worktree_path"
        workspace="$worktree_path"
        echo "Working directory: $workspace"
    fi

    echo "Workspace: $workspace"

    # Validate workspace path for security (prevent command injection)
    validate_workspace_path "$workspace"

    # Scan workspace for credentials if enabled (before any container operations)
    scan_credentials "$workspace" "$CRED_SCAN"

    # Get container name (workspace-scoped)
    local container_name
    container_name="$(get_container_name "$root_workspace" "$actual_worktree_name")"
    echo "Container name: $container_name"

    # Get cache volume name (use root workspace for sharing across worktrees)
    local cache_volume_name
    cache_volume_name="$(get_cache_volume_name "$root_workspace")"
    echo "Cache volume: $cache_volume_name"

    # Create cache volume if it doesn't exist
    create_cache_volume "$cache_volume_name"

    # Read Git config
    local git_config
    git_config="$(read_git_config)"
    local git_name="${git_config%|*}"
    local git_email="${git_config##*|}"

    if [ -n "$git_name" ]; then
        echo "Git user.name: $git_name"
    fi
    if [ -n "$git_email" ]; then
        echo "Git user.email: $git_email"
    fi

    # Get host Crush config path
    local host_config_path=""
    if [ "$NO_HOST_CONFIG" = "true" ]; then
        echo "Skipping host Crush config (--no-host-config)"
    elif [ -n "$CUSTOM_CONFIG_PATH" ]; then
        # Use custom config path
        if [ -d "$CUSTOM_CONFIG_PATH" ]; then
            if [ -r "$CUSTOM_CONFIG_PATH" ]; then
                host_config_path="$CUSTOM_CONFIG_PATH"
                echo "Using custom Crush config: $host_config_path"
            else
                echo "Error: Custom config directory exists but is not readable: $CUSTOM_CONFIG_PATH" >&2
                exit 1
            fi
        else
            echo "Warning: Custom config directory not found: $CUSTOM_CONFIG_PATH" >&2
        fi
    else
        host_config_path="$(get_host_crush_config_path)"
        if [ -n "$host_config_path" ]; then
            echo "Host Crush config: $host_config_path"
        fi
    fi

    # Get host Crush session data path
    local host_session_data_path=""
    if [ "$NO_HOST_CONFIG" = "true" ]; then
        echo "Skipping host Crush session data (--no-host-config)"
    else
        host_session_data_path="$(get_host_session_data_path)"
        if [ -n "$host_session_data_path" ]; then
            echo "Host Crush session data: $host_session_data_path"
        fi
    fi

    # Check if container exists, create if not
    if container_exists "$container_name"; then
        echo "Reusing existing container"
        # Read stored Docker args from container label for persistence
        local stored_docker_args
        stored_docker_args=$(docker inspect --format '{{index .Config.Labels "crush-sandbox.extra-docker-args"}}' "$container_name" 2>/dev/null || echo "")
        if [ -n "$stored_docker_args" ]; then
            # Decode base64-encoded Docker args
            stored_docker_args=$(echo "$stored_docker_args" | base64 -d 2>/dev/null || echo "")
            if [ -n "$stored_docker_args" ]; then
                echo "Using stored Docker args: $stored_docker_args"
            fi
        fi

        # Validate that provided Docker args match stored args
        # Changing Docker args requires container removal (via `clean` command)
        local requested_docker_args=""
        if [ -n "$DOCKER_ARGS_ENV" ]; then
            requested_docker_args="$DOCKER_ARGS_ENV"
        fi
        if [ -n "$DOCKER_ARGS" ]; then
            if [ -n "$requested_docker_args" ]; then
                requested_docker_args="$requested_docker_args $DOCKER_ARGS"
            else
                requested_docker_args="$DOCKER_ARGS"
            fi
        fi

        if [ -n "$requested_docker_args" ] && [ "$requested_docker_args" != "$stored_docker_args" ]; then
            echo "Docker args mismatch detected"
            echo "  Stored args:  $stored_docker_args"
            echo "  Requested args: $requested_docker_args"

            # Check if we're in TTY mode (interactive terminal)
            if [ -t 0 ]; then
                echo ""
                echo "Options:"
                echo "  1. Proceed with stored args ($stored_docker_args)"
                echo "  2. Clean container and recreate with new args ($requested_docker_args)"
                echo ""

                # Default to option 1 (use stored args)
                read -p "Enter choice [1]: " choice
                choice="${choice:-1}"

                case "$choice" in
                    1)
                        echo "Proceeding with stored Docker args: $stored_docker_args"
                        ;;
                    2)
                        echo "Cleaning container..."
                        docker stop "$container_name" > /dev/null 2>&1 || true
                        docker rm "$container_name" > /dev/null 2>&1
                        echo "Creating new container with args: $requested_docker_args"

                        # Merge args for creation
                        local merged_docker_args="$requested_docker_args"
                        local docker_args_source="cli"
                        if [ -n "$DOCKER_ARGS_ENV" ] && [ -n "$DOCKER_ARGS" ]; then
                            docker_args_source="both"
                        fi

                        create_container "$workspace" "$container_name" "$git_name" "$git_email" "$cache_volume_name" "$host_config_path" "$host_session_data_path" "$merged_docker_args" "$docker_args_source"
                        ;;
                    *)
                        echo "Invalid choice. Aborted."
                        exit 1
                        ;;
                esac
            else
                # Programmatic mode: silent success, use stored args
                echo "Using stored Docker args: $stored_docker_args"
            fi
        fi
    else
        echo "Creating new container"
        # Merge environment variable and CLI flag Docker args (env var + CLI flag)
        local merged_docker_args=""
        local docker_args_source="none"
        if [ -n "$DOCKER_ARGS_ENV" ]; then
            merged_docker_args="$DOCKER_ARGS_ENV"
            docker_args_source="env"
        fi
        if [ -n "$DOCKER_ARGS" ]; then
            if [ -n "$merged_docker_args" ]; then
                merged_docker_args="$merged_docker_args $DOCKER_ARGS"
                docker_args_source="both"
            else
                merged_docker_args="$DOCKER_ARGS"
                docker_args_source="cli"
            fi
        fi
        create_container "$workspace" "$container_name" "$git_name" "$git_email" "$cache_volume_name" "$host_config_path" "$host_session_data_path" "$merged_docker_args" "$docker_args_source"
    fi

    # Run the container
    run_container "$container_name" "$SHELL_MODE" "$host_config_path" "$workspace" "$PROGRAMATIC_MODE" "$PROMPT" "$MODEL"
}

clean_command() {
    local force="$1"

    # Validate Docker is available
    if ! validate_docker; then
        exit 1
    fi

    # Get workspace info (current directory)
    local workspace
    workspace="$(get_workspace_info)"

    # Get root workspace (git repository root - for cache sharing)
    local root_workspace
    if ! validate_git_repository; then
        echo "Error: Not in a git repository" >&2
        exit 1
    fi
    root_workspace="$(get_root_workspace)"
    if [ -z "$root_workspace" ]; then
        echo "Error: Could not determine root workspace" >&2
        exit 1
    fi

    # Extract worktree name if in a worktree
    local worktree_name
    worktree_name="$(extract_worktree_name "$workspace")"

    # Get container name (workspace-scoped)
    local container_name
    container_name="$(get_container_name "$root_workspace" "$worktree_name")"

    # Get cache volume name (repository-scoped for sharing)
    local cache_volume_name
    cache_volume_name="$(get_cache_volume_name "$root_workspace")"

    # Find all containers for this repository
    local all_containers
    all_containers="$(get_repository_containers "$root_workspace")"
    local container_count
    if [ -n "$all_containers" ]; then
        container_count=$(echo "$all_containers" | wc -l | tr -d ' ')
    else
        container_count=0
    fi

    # Prompt user for cleanup scope (unless --force is set)
    local clean_scope=""
    if [ "$force" != "true" ]; then
        echo ""
        echo "Cleanup options:"
        echo "  1. Clean current workspace container only"
        echo "  2. Clean all workspace containers in this repository ($container_count container(s))"
        echo ""

        # Default to option 1 (current workspace only)
        read -p "Enter choice [1]: " choice
        choice="${choice:-1}"

        case "$choice" in
            1)
                clean_scope="current"
                ;;
            2)
                clean_scope="all"
                ;;
            *)
                echo "Invalid choice. Aborted."
                exit 0
                ;;
        esac
    else
        # With --force flag, clean all containers and cache
        clean_scope="all"
    fi

    # Execute cleanup based on scope
    if [ "$clean_scope" = "current" ]; then
        # Clean current workspace container only
        echo ""
        echo "Cleaning current workspace container..."

        if ! container_exists "$container_name"; then
            echo "No sandbox container found for workspace: $workspace"
        else
            echo "Found container: $container_name"

            # Stop the container if it's running
            echo "Stopping container..."
            docker stop "$container_name" > /dev/null 2>&1 || true

            # Remove the container
            echo "Removing container..."
            if docker rm "$container_name" > /dev/null 2>&1; then
                print_success "Container removed: $container_name"
            else
                print_error "Failed to remove container: $container_name"
                exit 1
            fi
        fi

        # Do NOT remove cache volume for current workspace cleanup
        echo ""
        echo "Note: Cache volume $cache_volume_name was NOT removed"
        echo "      (shared across all workspaces in this repository)"

    elif [ "$clean_scope" = "all" ]; then
        # Clean all containers and cache volume
        echo ""
        echo "Cleaning all workspace containers and cache volume..."

        # Require confirmation for all cleanup (unless --force is set)
        if [ "$force" != "true" ]; then
            echo ""
            echo "This will remove:"
            if [ "$container_count" -gt 0 ]; then
                echo "  - $container_count container(s)"
            fi
            echo "  - Cache volume: $cache_volume_name"
            echo ""
            read -p "Are you sure? [y/N] " confirm
            if [[ ! "$confirm" =~ ^[yY] ]]; then
                echo "Aborted"
                exit 0
            fi
        fi

        # Remove all containers for this repository
        if [ -n "$all_containers" ]; then
            local i=1
            echo ""
            while IFS= read -r container; do
                echo "[$i/$container_count] Removing container $container..."

                # Stop the container if it's running
                docker stop "$container" > /dev/null 2>&1 || true

                # Remove the container
                if docker rm "$container" > /dev/null 2>&1; then
                    print_success "Removed: $container"
                else
                    print_error "Failed to remove: $container"
                fi

                i=$((i + 1))
            done <<< "$all_containers"
        else
            echo "No containers found"
        fi

        # Remove cache volume
        echo ""
        echo "Removing cache volume..."
        if docker volume rm "$cache_volume_name" > /dev/null 2>&1; then
            print_success "Cache volume removed: $cache_volume_name"
        else
            echo "Cache volume not found: $cache_volume_name"
        fi
    fi
}

install_command() {
    # Validate Docker is available before installing
    if ! validate_docker; then
        echo "Error: Docker is required to use crush-sandbox" >&2
        echo "Please install Docker Desktop and ensure it is running" >&2
        exit 1
    fi

    local install_path="/usr/local/bin/crush-sandbox"
    local alias_path="/usr/local/bin/crushbox"
    local remote_url="https://raw.githubusercontent.com/wireless25/crush-sandbox/main/docker-sandbox-crush"

    # Check if already installed
    if [ -f "$install_path" ]; then
        echo "crush-sandbox is already installed at $install_path"
        read -p "Do you want to overwrite it? [y/N] " confirm
        if [[ ! "$confirm" =~ ^[yY] ]]; then
            echo "Installation cancelled"
            exit 0
        fi
    fi

    # Check if we can write to /usr/local/bin
    if [ ! -w "$(dirname "$install_path")" ]; then
        echo "Error: Cannot write to $(dirname "$install_path"). Please run with sudo or choose a different location." >&2
        exit 1
    fi

    # Download the script
    echo "Downloading crush-sandbox from GitHub..."
    if ! curl -fsSL "$remote_url" -o "$install_path"; then
        echo "Error: Failed to download script from $remote_url" >&2
        exit 1
    fi

    # Make executable
    chmod +x "$install_path"

    # Create alias symlink
    echo "Creating alias 'crushbox'..."
    if [ -L "$alias_path" ] || [ -f "$alias_path" ]; then
        echo "Removing existing $alias_path..."
        rm -f "$alias_path"
    fi
    ln -s "$install_path" "$alias_path"
    chmod +x "$alias_path"

    echo ""
    echo "✓ Successfully installed crush-sandbox to $install_path"
    echo "✓ Alias 'crushbox' created at $alias_path"
    echo "✓ Docker validated: $(docker --version)"
    echo ""
    echo "Note: gitleaks Docker image will be pulled on first use for credential scanning"
    echo ""
    echo "You can now run:"
    echo "  crush-sandbox run      # Start Crush CLI in sandbox"
    echo "  crushbox run           # Alias: same as crush-sandbox run"
    echo "  crush-sandbox clean    # Remove sandbox for current workspace"
    echo "  crush-sandbox update   # Update to latest version"
    echo ""
    echo "For more information, run:"
    echo "  crush-sandbox --help"
}

update_command() {
    local script_path="${BASH_SOURCE[0]}"
    local remote_url="https://raw.githubusercontent.com/wireless25/crush-sandbox/main/docker-sandbox-crush"
    local temp_file

    # Use mktemp for secure temporary file creation (TOCTOU race condition prevention)
    temp_file=$(mktemp) || {
        echo "Error: Failed to create temporary file" >&2
        exit 1
    }

    echo "Checking for updates..."

    # Download latest version to temp file
    echo "Downloading latest version from GitHub..."
    if ! curl -fsSL "$remote_url" -o "$temp_file"; then
        echo "Error: Failed to download latest version from $remote_url" >&2
        rm -f "$temp_file"
        exit 1
    fi

    # Make executable so we can validate it
    chmod +x "$temp_file"

    # Extract version from downloaded file using grep and cut
    local remote_version
    remote_version=$(grep '^VERSION=' "$temp_file" | head -1 | cut -d'"' -f2)
    if [ -z "$remote_version" ]; then
        echo "Error: Failed to determine remote version" >&2
        rm -f "$temp_file"
        exit 1
    fi

    echo "Current version: $VERSION"
    echo "Latest version:  $remote_version"

    # Check if already up to date
    if [ "$remote_version" = "$VERSION" ]; then
        echo ""
        echo "✓ You already have the latest version!"
        rm -f "$temp_file"
        exit 0
    fi

    # Confirm update
    echo ""
    read -p "Update to version $remote_version? [y/N] " confirm
    if [[ ! "$confirm" =~ ^[yY] ]]; then
        echo "Update cancelled"
        rm -f "$temp_file"
        exit 0
    fi

    # Validate that the downloaded script is valid bash
    if ! bash -n "$temp_file"; then
        echo "Error: Downloaded script has syntax errors, not updating" >&2
        rm -f "$temp_file"
        exit 1
    fi

    # Replace current script with updated version
    echo ""
    echo "Updating crush-sandbox..."
    if ! mv "$temp_file" "$script_path"; then
        echo "Error: Failed to update script. Try running with sudo." >&2
        rm -f "$temp_file"
        exit 1
    fi

    # Make executable
    chmod +x "$script_path"

    echo ""
    echo "✓ Successfully updated from $VERSION to $remote_version"
    echo "  Script location: $script_path"
    echo ""
    echo "Changes since your version:"
    echo "View release notes: https://github.com/wireless25/crush-sandbox/releases"
}

uninstall_command() {
    local dry_run="$1"
    local force="$2"

    # Track failures for summary
    local script_failed=false
    local alias_failed=false
    local container_failures=()
    local volume_failures=()

    echo "=== Docker Sandbox Crush Uninstall ==="
    echo ""

    # Validate Docker is available (even for dry-run, to detect containers)
    if ! validate_docker; then
        echo "Error: Docker is required to uninstall containers and volumes" >&2
        exit 1
    fi

    # Detect running containers
    local running_containers
    running_containers=$(docker ps --filter "name=^crush-sandbox-" --format '{{.Names}}' 2>/dev/null || echo "")

    if [ -n "$running_containers" ]; then
        local count
        count=$(echo "$running_containers" | wc -l | tr -d ' ')
        print_warning "$count running container(s) found:"
        echo "$running_containers" | while read -r container; do
            echo "  - $container"
        done
        echo ""

        if [ "$dry_run" = "true" ]; then
            print_dry_run "Would continue with uninstall despite running containers"
        else
            read -p "Continue anyway? [y/N] " confirm
            if [[ ! "$confirm" =~ ^[yY] ]]; then
                echo "Uninstall cancelled"
                exit 2
            fi
        fi
    fi

    # Step 1: Remove script
    local script_paths=("/usr/local/bin/crush-sandbox" "/usr/local/bin/docker-sandbox-crush")
    local script_to_remove=""
    for script_path in "${script_paths[@]}"; do
        if [ -f "$script_path" ]; then
            script_to_remove="$script_path"
            break
        fi
    done

    if [ -n "$script_to_remove" ]; then
        print_info "Script: $script_to_remove"

        if [ "$dry_run" = "true" ]; then
            print_dry_run "Would remove script at $script_to_remove"
        elif [ "$force" = "true" ]; then
            if sudo rm -f "$script_to_remove" 2>/dev/null; then
                print_success "Removed script at $script_to_remove"
            else
                print_error "Failed to remove script at $script_to_remove"
                script_failed=true
            fi
        else
            read -p "Remove script at $script_to_remove? [y/N] " confirm
            if [[ "$confirm" =~ ^[yY] ]]; then
                if sudo rm -f "$script_to_remove" 2>/dev/null; then
                    print_success "Removed script at $script_to_remove"
                else
                    print_error "Failed to remove script at $script_to_remove"
                    script_failed=true
                fi
            else
                echo "Skipped script removal"
            fi
        fi
    else
        echo "No script found (expected at /usr/local/bin/crush-sandbox or /usr/local/bin/docker-sandbox-crush)"
    fi

    echo ""

    # Step 2: Remove alias
    local alias_path="/usr/local/bin/crushbox"
    if [ -f "$alias_path" ] || [ -L "$alias_path" ]; then
        print_info "Alias: $alias_path"

        if [ "$dry_run" = "true" ]; then
            print_dry_run "Would remove alias at $alias_path"
        elif [ "$force" = "true" ]; then
            if sudo rm -f "$alias_path" 2>/dev/null; then
                print_success "Removed alias at $alias_path"
            else
                print_error "Failed to remove alias at $alias_path"
                alias_failed=true
            fi
        else
            read -p "Remove alias at $alias_path? [y/N] " confirm
            if [[ "$confirm" =~ ^[yY] ]]; then
                if sudo rm -f "$alias_path" 2>/dev/null; then
                    print_success "Removed alias at $alias_path"
                else
                    print_error "Failed to remove alias at $alias_path"
                    alias_failed=true
                fi
            else
                echo "Skipped alias removal"
            fi
        fi
    else
        echo "No alias found at $alias_path"
    fi

    echo ""

    # Step 3: Remove containers
    local containers
    containers=$(docker ps -a --filter "name=^crush-sandbox-" --format '{{.Names}}' 2>/dev/null || echo "")

    if [ -n "$containers" ]; then
        local container_count
        container_count=$(echo "$containers" | wc -l | tr -d ' ')
        echo "Found $container_count container(s):"

        # Use a different approach to avoid subshell issues
        while IFS= read -r container; do
            local status
            status=$(docker inspect --format '{{.State.Status}}' "$container" 2>/dev/null || echo "unknown")
            echo "  - $container ($status)"
        done <<< "$containers"

        if [ "$dry_run" = "true" ]; then
            print_dry_run "Would remove $container_count container(s)"
            echo ""
        elif [ "$force" = "true" ] || [[ "$(read -p "Remove $container_count container(s)? [y/N] " confirm; echo "$confirm")" =~ ^[yY] ]]; then
            if [ "$force" != "true" ]; then
                read -p "Remove $container_count container(s)? [y/N] " confirm
                if [[ ! "$confirm" =~ ^[yY] ]]; then
                    echo "Skipped container removal"
                else
                    echo ""
                fi
            else
                echo ""
            fi

            if [ "$force" = "true" ] || [[ "$confirm" =~ ^[yY] ]]; then
                local i=1
                while IFS= read -r container; do
                    echo -n "[$i/$container_count] Removing container $container... "
                    if docker stop "$container" >/dev/null 2>&1; then
                        if docker rm "$container" >/dev/null 2>&1; then
                            print_success "Removed $container"
                        else
                            print_error "Failed to remove $container (stop succeeded)"
                            container_failures+=("Failed to remove $container")
                        fi
                    else
                        print_error "Failed to stop $container"
                        container_failures+=("Failed to stop $container")
                    fi
                    i=$((i + 1))
                done <<< "$containers"
            fi
        else
            echo "Skipped container removal"
        fi
    else
        echo "No containers found"
    fi

    echo ""

    # Step 4: Remove volumes
    local volumes
    volumes=$(docker volume ls --filter "name=^crush-cache-" --format '{{.Name}}' 2>/dev/null || echo "")

    if [ -n "$volumes" ]; then
        local volume_count
        volume_count=$(echo "$volumes" | wc -l | tr -d ' ')
        echo "Found $volume_count volume(s):"

        # Use a different approach to avoid subshell issues
        while IFS= read -r volume; do
            local size
            size=$(docker volume inspect --format '{{.UsageData.Size}}' "$volume" 2>/dev/null || echo "unknown")
            echo "  - $volume ($size)"
        done <<< "$volumes"

        if [ "$dry_run" = "true" ]; then
            print_dry_run "Would remove $volume_count volume(s)"
            echo ""
        elif [ "$force" = "true" ] || [[ "$(read -p "Remove $volume_count volume(s)? [y/N] " confirm; echo "$confirm")" =~ ^[yY] ]]; then
            if [ "$force" != "true" ]; then
                read -p "Remove $volume_count volume(s)? [y/N] " confirm
                if [[ ! "$confirm" =~ ^[yY] ]]; then
                    echo "Skipped volume removal"
                else
                    echo ""
                fi
            else
                echo ""
            fi

            if [ "$force" = "true" ] || [[ "$confirm" =~ ^[yY] ]]; then
                local i=1
                while IFS= read -r volume; do
                    echo -n "[$i/$volume_count] Removing volume $volume... "
                    if docker volume rm "$volume" >/dev/null 2>&1; then
                        print_success "Removed $volume"
                    else
                        print_error "Failed to remove $volume"
                        volume_failures+=("Failed to remove $volume")
                    fi
                    i=$((i + 1))
                done <<< "$volumes"
            fi
        else
            echo "Skipped volume removal"
        fi
    else
        echo "No volumes found"
    fi

    echo ""

    # Final summary
    echo "=== Uninstall Summary ==="

    # Script status
    if [ -n "$script_to_remove" ]; then
        if [ "$dry_run" = "true" ]; then
            echo -e "Script: ${YELLOW}[DRY RUN]${NC} Would remove $script_to_remove"
        elif [ "$script_failed" = "true" ]; then
            echo -e "Script: ${RED}✗ Failed${NC}"
        else
            echo -e "Script: ${GREEN}✓ Removed${NC}"
        fi
    else
        echo -e "Script: ${GRAY}N/A${NC} (not installed)"
    fi

    # Alias status
    if [ -f "$alias_path" ] || [ -L "$alias_path" ]; then
        if [ "$dry_run" = "true" ]; then
            echo -e "Alias: ${YELLOW}[DRY RUN]${NC} Would remove $alias_path"
        elif [ "$alias_failed" = "true" ]; then
            echo -e "Alias: ${RED}✗ Failed${NC}"
        else
            echo -e "Alias: ${GREEN}✓ Removed${NC}"
        fi
    else
        echo -e "Alias: ${GRAY}N/A${NC} (not installed)"
    fi

    # Containers status
    if [ -n "$containers" ]; then
        local total_containers
        total_containers=$(echo "$containers" | wc -l | tr -d ' ')
        local container_fail_count
        if [ "$dry_run" = "true" ]; then
            container_fail_count=0
            echo -e "Containers: ${YELLOW}[DRY RUN]${NC} Would remove $total_containers container(s)"
        else
            container_fail_count=${#container_failures[@]}
            if [ $container_fail_count -eq 0 ]; then
                echo -e "Containers: ${GREEN}✓ Removed all${NC} ($total_containers)"
            else
                echo -e "Containers: ${RED}✗ Partially failed${NC} ($container_fail_count/$total_containers failed)"
            fi
        fi
    else
        echo -e "Containers: ${GRAY}N/A${NC} (none found)"
    fi

    # Volumes status
    if [ -n "$volumes" ]; then
        local total_volumes
        total_volumes=$(echo "$volumes" | wc -l | tr -d ' ')
        local volume_fail_count
        if [ "$dry_run" = "true" ]; then
            volume_fail_count=0
            echo -e "Volumes: ${YELLOW}[DRY RUN]${NC} Would remove $total_volumes volume(s)"
        else
            volume_fail_count=${#volume_failures[@]}
            if [ $volume_fail_count -eq 0 ]; then
                echo -e "Volumes: ${GREEN}✓ Removed all${NC} ($total_volumes)"
            else
                echo -e "Volumes: ${RED}✗ Partially failed${NC} ($volume_fail_count/$total_volumes failed)"
            fi
        fi
    else
        echo -e "Volumes: ${GRAY}N/A${NC} (none found)"
    fi

    # Error details
    if [ "$dry_run" != "true" ]; then
        local total_errors=0
        [ "$script_failed" = "true" ] && total_errors=$((total_errors + 1))
        [ "$alias_failed" = "true" ] && total_errors=$((total_errors + 1))
        total_errors=$((total_errors + ${#container_failures[@]} + ${#volume_failures[@]}))

        if [ $total_errors -gt 0 ]; then
            echo ""
            echo "=== Errors ==="

            if [ "$script_failed" = "true" ]; then
                echo -e "  ${RED}✗${NC} Script: Failed to remove"
            fi

            if [ "$alias_failed" = "true" ]; then
                echo -e "  ${RED}✗${NC} Alias: Failed to remove"
            fi

            for failure in "${container_failures[@]}"; do
                echo -e "  ${RED}✗${NC} Container: $failure"
            done

            for failure in "${volume_failures[@]}"; do
                echo -e "  ${RED}✗${NC} Volume: $failure"
            done

            echo ""
            echo -e "${RED}Uninstall complete. $total_errors error(s) occurred.${NC}"
            exit 1
        else
            echo ""
            echo -e "${GREEN}Uninstall complete. All components removed successfully.${NC}"
            exit 0
        fi
    else
        echo ""
        echo -e "${YELLOW}[DRY RUN] Uninstall preview complete. No changes made.${NC}"
        exit 0
    fi
}

# Git worktree functions
get_root_workspace() {
    local workspace
    local common_dir
    # Use --git-common-dir instead of --show-toplevel because:
    # - --show-toplevel returns worktree path when inside a worktree (not the actual repo root)
    # - --git-common-dir returns the common .git directory for all worktrees (always the actual repo root)
    common_dir="$(git rev-parse --git-common-dir 2>/dev/null)"

    # Remove .git suffix to get the workspace root
    # Handle both "/path/.git" and ".git" formats
    if [[ "$common_dir" == */.git ]]; then
        workspace="${common_dir%/.git}"
    elif [[ "$common_dir" == ".git" ]]; then
        workspace="."
    else
        workspace="$common_dir"
    fi

    # Resolve relative path to absolute path (git-common-dir can return relative path)
    if [[ "$workspace" != /* ]]; then
        # cd to workspace and get absolute path
        workspace="$(cd "$workspace" && pwd -P)"
    fi

    echo "$workspace"
}

validate_git_repository() {
    if ! git rev-parse --git-dir >/dev/null 2>&1; then
        echo "Error: Not in a git repository" >&2
        return 1
    fi
    return 0
}

validate_worktree_branch_args() {
    if [ -n "$BRANCH_NAME" ] && [ "$WORKTREE_MODE" != "true" ]; then
        echo "Error: --branch-name requires --worktree" >&2
        return 1
    fi
    return 0
}

extract_worktree_name() {
    local workspace="$1"
    local worktree_name=""

    case "$workspace" in
        */.worktrees/*)
            # Extract worktree name from path (directory after .worktrees/)
            worktree_name="${workspace#*/.worktrees/}"
            # Remove any trailing path components (in case of nested paths)
            worktree_name="${worktree_name%%/*}"
            ;;
    esac

    echo "$worktree_name"
}

prepare_worktree_directory() {
    local workspace="$1"

    # Create .worktrees directory if it doesn't exist
    if [ ! -d "${workspace}/.worktrees" ]; then
        mkdir -p "${workspace}/.worktrees"
        echo "Created .worktrees directory"
    fi

    # Create .worktrees/.gitignore if it doesn't exist
    if [ ! -f "${workspace}/.worktrees/.gitignore" ]; then
        echo "*" > "${workspace}/.worktrees/.gitignore"
        echo "Created .worktrees/.gitignore"
    fi
}

generate_worktree_name() {
    local name
    # Generate 8-character alphanumeric name
    name=$(cat /dev/urandom | LC_ALL=C tr -dc 'a-zA-Z0-9' | fold -w 8 | head -n 1)
    echo "$name"
}

worktree_exists() {
    local workspace="$1"
    local worktree_name="$2"
    local expected_path="${workspace}/.worktrees/${worktree_name}"

    # Check if worktree with this name exists
    # Use awk to extract just the path for comparison
    if git worktree list | awk '{print $1}' | grep -q "^${expected_path}$"; then
        return 0
    fi
    return 1
}

create_worktree() {
    local workspace="$1"
    local worktree_name="$2"
    local worktree_path="${workspace}/.worktrees/${worktree_name}"
    local current_branch
    current_branch="$(git branch --show-current)"

    # Check if worktree already exists
    if worktree_exists "$workspace" "$worktree_name"; then
        echo "Error: Worktree '${worktree_name}' already exists" >&2
        echo "Use 'crush-sandbox list-worktrees' to see all worktrees" >&2
        return 1
    fi

    # Determine branch to use
    local branch=""
    if [ -n "$3" ]; then
        branch="$3"
    elif [ -n "$current_branch" ]; then
        # Check if current branch is checked out in a worktree different from workspace
        local current_path
        current_path="$(pwd)"
        if [ "$current_path" != "$workspace" ]; then
            # We're in a worktree, current branch is available for new worktree
            branch="$current_branch"
        else
            # We're in main workspace, current branch is already checked out
            branch=""
        fi
    fi

    # Prompt for branch if not determined
    if [ -z "$branch" ]; then
        # Check if we're in non-TTY mode
        if [ ! -t 0 ]; then
            # Non-TTY mode: use worktree name as branch name
            branch="$worktree_name"
        else
            # Interactive mode: prompt for branch name
            echo ""
            echo "Branch to checkout in worktree:"
            echo "  - Enter an existing branch name (e.g., 'feature/new-thing')"
            echo "  - Enter a new branch name to create it"
            read -p "Branch name (press Enter to use '$worktree_name'): " input_branch
            if [ -z "$input_branch" ]; then
                branch="$worktree_name"
            else
                branch="$input_branch"
            fi
        fi
    fi

    # Create the worktree
    # Check if branch exists to determine correct git worktree add syntax
    if git show-ref --verify "refs/heads/$branch" >/dev/null 2>&1; then
        # Branch exists, checkout it in new worktree
        if git worktree add "$worktree_path" "$branch" 2>&1; then
            echo ""
            print_success "Worktree created: $worktree_path"
            echo "Branch: $branch"
            echo ""
            return 0
        else
            echo "Error: Failed to create worktree" >&2
            return 1
        fi
    else
        # Branch doesn't exist, create it in new worktree
        if git worktree add -b "$branch" "$worktree_path" 2>&1; then
            echo ""
            print_success "Worktree created: $worktree_path"
            echo "Branch: $branch (new branch created)"
            echo ""
            return 0
        else
            echo "Error: Failed to create worktree" >&2
            return 1
        fi
    fi
}

list_worktrees() {
    local current_dir
    current_dir="$(pwd)"

    # Get list of worktrees
    local worktrees
    worktrees=$(git worktree list 2>/dev/null || echo "")

    if [ -z "$worktrees" ]; then
        echo "No worktrees found"
        return 0
    fi

    echo "Worktrees:"
    echo ""

    while IFS= read -r line; do
        local worktree_path
        local branch
        local current_indicator=""

        worktree_path=$(echo "$line" | awk '{print $1}')
        branch=$(echo "$line" | sed 's/.*\[//' | sed 's/\].*//' 2>/dev/null || echo "unknown")

        # Mark current worktree
        if [ "$worktree_path" = "$current_dir" ]; then
            current_indicator=" (current)"
        fi

        echo "  - ${worktree_path}${current_indicator}"
        echo "    Branch: ${branch}"
        echo ""
    done <<< "$worktrees"
}

remove_worktree() {
    local workspace="$1"
    local worktree_name="$2"
    local force_flag="$3"
    local worktree_path="${workspace}/.worktrees/${worktree_name}"

    # Check if worktree exists
    if ! worktree_exists "$workspace" "$worktree_name"; then
        echo "Error: Worktree '${worktree_name}' does not exist" >&2
        echo "Use 'crush-sandbox list-worktrees' to see all worktrees" >&2
        return 1
    fi

    # Build git worktree remove command
    local remove_cmd="git worktree remove"
    if [ "$force_flag" = "true" ]; then
        remove_cmd="$remove_cmd --force"
    fi
    remove_cmd="$remove_cmd $worktree_path"

    # Remove the worktree
    if $remove_cmd 2>&1; then
        print_success "Worktree removed: $worktree_path"
        return 0
    else
        echo "Error: Failed to remove worktree" >&2
        if [ "$force_flag" != "true" ]; then
            echo "Hint: Use --force to remove worktree with uncommitted changes" >&2
        fi
        return 1
    fi
}

list_containers() {
    local current_dir
    current_dir="$(pwd)"

    # Validate git repository
    if ! validate_git_repository; then
        exit 1
    fi

    # Get root workspace
    local root_workspace
    root_workspace="$(get_root_workspace)"
    if [ -z "$root_workspace" ]; then
        echo "Error: Could not determine root workspace" >&2
        exit 1
    fi

    # Get all containers for this repository
    local containers
    containers=$(get_repository_containers "$root_workspace")

    if [ -z "$containers" ]; then
        echo "No containers found"
        return 0
    fi

    echo "Containers:"
    echo ""

    while IFS= read -r container; do
        # Extract workspace path from container name
        local workspace_path
        local worktree_name
        local current_indicator=""

        # Determine workspace path from container name
        # Container name pattern: crush-sandbox-<repo-hash> or crush-sandbox-<repo-hash>-<worktree-name>
        local repo_hash
        repo_hash="$(echo -n "$root_workspace" | shasum -a 256 | awk '{print $1}' | cut -c1-12)"

        if [[ "$container" == "crush-sandbox-${repo_hash}" ]]; then
            # Main workspace container
            workspace_path="$root_workspace"
            worktree_name=""
        else
            # Worktree container - extract worktree name from container name
            worktree_name="${container#crush-sandbox-${repo_hash}-}"
            workspace_path="${root_workspace}/.worktrees/${worktree_name}"
        fi

        # Get branch name for this workspace
        local branch="unknown"
        if [ -d "$workspace_path" ]; then
            branch=$(cd "$workspace_path" && git branch --show-current 2>/dev/null || echo "unknown")
        fi

        # Get container status
        local status
        status=$(docker inspect --format '{{.State.Status}}' "$container" 2>/dev/null || echo "nonexistent")

        # Mark current workspace
        if [ "$workspace_path" = "$current_dir" ]; then
            current_indicator=" (current)"
        fi

        # Display container info with status indicator
        local status_indicator
        case "$status" in
            running)
                status_indicator="${GREEN}●${NC}"
                ;;
            exited|stopped)
                status_indicator="${RED}●${NC}"
                ;;
            *)
                status_indicator="${GRAY}●${NC}"
                ;;
        esac

        echo "  ${status_indicator} ${workspace_path}${current_indicator}"
        echo "    Branch: ${branch}"
        echo "    Container: ${container}"
        echo "    Status: ${status}"
        echo ""
    done <<< "$containers"
}

# Main script logic (only run if script is executed directly, not sourced)
# Check if being sourced by comparing BASH_SOURCE vs $0
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
    if [ $# -lt 1 ]; then
        print_usage >&2
        exit 1
    fi

# Parse arguments
COMMAND=""
FORCE="false"
SHELL_MODE="false"
SHOW_VERSION="false"
NO_HOST_CONFIG="false"
CRED_SCAN="false"
DRY_RUN="false"
WORKTREE_MODE="false"
WORKTREE_NAME=""
BRANCH_NAME=""
LIST_WORKTREES="false"
REMOVE_WORKTREE=""
PROGRAMATIC_MODE="false"
PROMPT=""
MODEL=""
DOCKER_ARGS=""
DOCKER_ARGS_ENV=""

# Read CRUSH_SANDBOX_EXTRA_DOCKER_ARGS environment variable
if [ -n "$CRUSH_SANDBOX_EXTRA_DOCKER_ARGS" ]; then
    DOCKER_ARGS_ENV="$CRUSH_SANDBOX_EXTRA_DOCKER_ARGS"
fi

while [[ $# -gt 0 ]]; do
    case "$1" in
        --version)
            SHOW_VERSION="true"
            shift
            ;;
        --force)
            FORCE="true"
            shift
            ;;
        --shell)
            SHELL_MODE="true"
            shift
            ;;
        --no-host-config)
            NO_HOST_CONFIG="true"
            shift
            ;;
        --cred-scan)
            CRED_SCAN="true"
            shift
            ;;
        --dry-run)
            DRY_RUN="true"
            shift
            ;;
        --worktree)
            WORKTREE_MODE="true"
            if [[ -n "$2" && ! "$2" =~ ^- ]]; then
                WORKTREE_NAME="$2"
                shift 2
            else
                WORKTREE_NAME=""
                shift
            fi
            ;;
        --branch-name)
            if [[ -z "$2" || "$2" =~ ^- ]]; then
                echo "Error: --branch-name requires a value" >&2
                exit 1
            fi
            BRANCH_NAME="$2"
            shift 2
            ;;
        -p)
            if [[ -z "$2" || "$2" =~ ^- ]]; then
                echo "Error: -p requires a value" >&2
                exit 1
            fi
            PROMPT="$2"
            PROGRAMATIC_MODE="true"
            shift 2
            ;;
        --model)
            if [[ -z "$2" || "$2" =~ ^- ]]; then
                echo "Error: --model requires a value" >&2
                exit 1
            fi
            MODEL="$2"
            shift 2
            ;;
        --docker-args)
            if [[ -z "$2" ]]; then
                echo "Error: --docker-args requires a value" >&2
                exit 1
            fi
            DOCKER_ARGS="$2"
            shift 2
            ;;
        --list-worktrees)
            LIST_WORKTREES="true"
            shift
            ;;
        -*)
            echo "Error: Unknown option: $1" >&2
            print_usage
            exit 1
            ;;
        *)
            if [ -z "$COMMAND" ]; then
                COMMAND="$1"
            elif [ "$COMMAND" = "remove-worktree" ] && [ -z "$REMOVE_WORKTREE" ]; then
                # Allow worktree name as argument to remove-worktree command
                REMOVE_WORKTREE="$1"
            else
                echo "Error: Unexpected argument: $1" >&2
                print_usage
                exit 1
            fi
            shift
            ;;
    esac
done

# Detect piped stdin input (US-002)
# Check if stdin is a pipe or redirected from file
if [ "$PROGRAMATIC_MODE" != "true" ]; then
    if [ -p /dev/stdin ]; then
        # Stdin is a pipe (e.g., echo "text" | ./script)
        PROMPT=$(cat)
        PROGRAMATIC_MODE="true"
    elif [ -f /dev/stdin ]; then
        # Stdin is redirected from file (e.g., heredocs)
        PROMPT=$(cat)
        PROGRAMATIC_MODE="true"
    fi
fi

# Show version and exit if requested
if [ "$SHOW_VERSION" = "true" ]; then
    echo "crush-sandbox version $VERSION"
    exit 0
fi

if [ -z "$COMMAND" ]; then
    print_usage >&2
    exit 1
fi

if ! validate_worktree_branch_args; then
    exit 1
fi

case "$COMMAND" in
    run)
        run_command
        ;;
    clean|reset)
        clean_command "$FORCE"
        ;;
    help)
        print_usage
        exit 0
        ;;
    install)
        install_command
        ;;
    update)
        update_command
        ;;
    uninstall)
        uninstall_command "$DRY_RUN" "$FORCE"
        ;;
    list-containers)
        list_containers
        exit 0
        ;;
    list-worktrees)
        # Validate git repository
        if ! validate_git_repository; then
            exit 1
        fi
        list_worktrees
        exit 0
        ;;
    remove-worktree)
        # Validate git repository
        if ! validate_git_repository; then
            exit 1
        fi
        # Get root workspace
        root_workspace="$(get_root_workspace)"
        if [ -z "$root_workspace" ]; then
            echo "Error: Could not determine root workspace" >&2
            exit 1
        fi
        # Remove worktree
        if ! remove_worktree "$root_workspace" "$REMOVE_WORKTREE" "$FORCE"; then
            exit 1
        fi
        exit 0
        ;;
    *)
        echo "Error: Unknown command: $COMMAND" >&2
        print_usage >&2
        exit 1
        ;;
esac

fi
